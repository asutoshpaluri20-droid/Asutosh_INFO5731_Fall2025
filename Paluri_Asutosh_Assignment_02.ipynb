{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeVnfvKBCkb8YxvA1VEzXX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asutoshpaluri20-droid/Asutosh_INFO5731_Fall2025/blob/main/Paluri_Asutosh_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ],
      "metadata": {
        "id": "IWOrD-nmDOLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ],
      "metadata": {
        "id": "mtaVH0XOT0vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pandas\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "dt = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "df = pd.DataFrame(dt[\"train\"])\n",
        "\n",
        "\n",
        "df_1000 = df.head(1000)[[\"text\"]]\n",
        "\n",
        "\n",
        "df_1000.to_csv(\"imdb_reviews.csv\", index=False)\n",
        "\n",
        "print(\"Collected and saved 1000 IMDb reviews to imdb_reviews.csv\")\n"
      ],
      "metadata": {
        "id": "DGMMPp40T0L1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b964509-8997-4e52-a773-6d1366e76870"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "✅ Collected and saved 1000 IMDb reviews to imdb_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"imdb_reviews.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cT1fbiuukoX9",
        "outputId": "03d3d31c-db5e-422a-f1bd-2dd2f7409cac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_24bb7ab5-2da8-45a1-a966-398ba082bcdc\", \"imdb_reviews.csv\", 1317136)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tf1IYAuCne5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "df = pd.read_csv(\"imdb_reviews.csv\")\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "df.to_csv(\"imdb_reviews_clean.csv\", index=False)\n",
        "\n",
        "print(\"Cleaned text data saved to imdb_reviews_clean.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "SwxXpC7SqBkF",
        "outputId": "b85a84b6-78e6-4d6f-dec2-4febe56bf4d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text data saved to imdb_reviews_clean.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  I rented I AM CURIOUS-YELLOW from my video sto...   \n",
              "1  \"I Am Curious: Yellow\" is a risible and preten...   \n",
              "2  If only to avoid making this type of film in t...   \n",
              "3  This film was probably inspired by Godard's Ma...   \n",
              "4  Oh, brother...after hearing about this ridicul...   \n",
              "\n",
              "                                          clean_text  \n",
              "0  rent curiousyellow video store controversi sur...  \n",
              "1  curiou yellow risibl pretenti steam pile doesn...  \n",
              "2  avoid make type film futur film interest exper...  \n",
              "3  film probabl inspir godard masculin féminin ur...  \n",
              "4  oh brotheraft hear ridicul film umpteen year t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3067de97-3a12-4692-973e-d1622aa0eb5e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
              "      <td>rent curiousyellow video store controversi sur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
              "      <td>curiou yellow risibl pretenti steam pile doesn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If only to avoid making this type of film in t...</td>\n",
              "      <td>avoid make type film futur film interest exper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This film was probably inspired by Godard's Ma...</td>\n",
              "      <td>film probabl inspir godard masculin féminin ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
              "      <td>oh brotheraft hear ridicul film umpteen year t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3067de97-3a12-4692-973e-d1622aa0eb5e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3067de97-3a12-4692-973e-d1622aa0eb5e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3067de97-3a12-4692-973e-d1622aa0eb5e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9b515d41-ed19-413c-b88d-5b0c68ba9b58\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b515d41-ed19-413c-b88d-5b0c68ba9b58')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9b515d41-ed19-413c-b88d-5b0c68ba9b58 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 997,\n        \"samples\": [\n          \"We don't have this on television in England but I walked it over the Internet on YouTube. It's dumb, immature and boring! This is from the creator of \\\"Earthworm Jim\\\" Douglas TenNapel, I never got into that cartoon but I must admit it better than this. The cartoonist hasn't done anything for years since now. For Doug TenNapel, this is a comeback travesty and an all time low! The story is about three cats who inherit a house and lots of money off their dead old lady master. They are argumentative and keep on disagreeing on what their want to spend their money on. \\\"BORING\\\"! The animation is dreadful. The main characters are meant to be cats, right? But they don't look nothing like cats! Just weird animal monster-looking creatures with big mouths, pointed teeth and bulgy eyes! The human and other animal characters are also drawn real ugly! The theme song is terrible and irritating! Also the stories are lame and are most probably copied from older shows. It surprised me how this show got 7.5/10 votes of other IMDb viewers. Television really isn't what is used to be! But now most of them is dumb, cheaply made and boring. Some of you on the website might not agree with me well I'm sorry but this is a total waste of money and a complete and utter waste of your time and feel glad that Britain don't have too tolerate this crap (oh yeah, if you have digital you have to) but I don't, so it not my problem! Loser! 2/10 (and it's very lucky to get that because I've given other shows worst!).\",\n          \"Instead, go to the zoo, buy some peanuts and feed 'em to the monkeys. Monkeys are funny. People with amnesia who don't say much, just sit there with vacant eyes are not all that funny.<br /><br />Black comedy? There isn't a black person in it, and there isn't one funny thing in it either.<br /><br />Walmart buys these things up somehow and puts them on their dollar rack. It's labeled Unrated. I think they took out the topless scene. They may have taken out other stuff too, who knows? All we know is that whatever they took out, isn't there any more.<br /><br />The acting seemed OK to me. There's a lot of unfathomables tho. It's supposed to be a city? It's supposed to be a big lake? If it's so hot in the church people are fanning themselves, why are they all wearing coats?\",\n          \"A decent sequel, but does not pack the punch of the original. A murderous screenwriter(Judd Nelson)assumes new identities in order to direct his own novel CABIN BY THE LAKE. Still ruthless killing, but movie seems very tongue-in-cheek. Any humor is not of the funny kind. Total project seems to have the quality of a quickie and at times Nelson is way over the top. This movie is about a script being rewritten before going to the screen...this should have happened to this script.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 997,\n        \"samples\": [\n          \"dont televis england walk internet youtub dumb immatur bore creator earthworm jim dougla tennapel never got cartoon must admit better cartoonist hasnt done anyth year sinc doug tennapel comeback travesti time low stori three cat inherit hous lot money dead old ladi master argument keep disagre want spend money bore anim dread main charact meant cat right dont look noth like cat weird anim monsterlook creatur big mouth point teeth bulgi eye human anim charact also drawn real ugli theme song terribl irrit also stori lame probabl copi older show surpris show got vote imdb viewer televis realli isnt use dumb cheapli made bore websit might agre well im sorri total wast money complet utter wast time feel glad britain dont toler crap oh yeah digit dont problem loser lucki get ive given show worst\",\n          \"instead go zoo buy peanut feed em monkey monkey funni peopl amnesia dont say much sit vacant eye funnybr br black comedi isnt black person isnt one funni thing eitherbr br walmart buy thing somehow put dollar rack label unrat think took topless scene may taken stuff know know whatev took isnt morebr br act seem ok there lot unfathom tho suppos citi suppos big lake hot church peopl fan wear coat\",\n          \"decent sequel pack punch origin murder screenwriterjudd nelsonassum new ident order direct novel cabin lake still ruthless kill movi seem tongueincheek humor funni kind total project seem qualiti quicki time nelson way top movi script rewritten go screenthi happen script\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ],
      "metadata": {
        "id": "bt0CEFKdqHyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ],
      "metadata": {
        "id": "dYm55Z46qsEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "df = pd.read_csv(\"imdb_reviews_clean.csv\")\n",
        "\n",
        "sample_reviews = df[\"clean_text\"].dropna().head(200).tolist()\n",
        "\n",
        "pos_counts = Counter()\n",
        "\n",
        "for review in sample_reviews:\n",
        "  doc = nlp(review)\n",
        "  for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
        "      pos_counts[token.pos_] += 1\n",
        "\n",
        "print(\"POS Tagger Counts:\")\n",
        "print(pos_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6KUqBg_qNmm",
        "outputId": "04699a7e-778a-4d87-f99e-5d7b702d1bd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagger Counts:\n",
            "Counter({'NOUN': 7546, 'VERB': 3487, 'ADJ': 2736, 'ADV': 912})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constituency Parsing and Dependency Prsing"
      ],
      "metadata": {
        "id": "7NI3jqZQqu5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import benepar\n",
        "import spacy\n",
        "\n",
        "# Add benepar (for constituency parsing)\n",
        "if spacy.__version__.startswith(\"3\"):\n",
        "    benepar.download('benepar_en3') # Downloading the model\n",
        "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "example_sentence = sample_reviews[0]\n",
        "doc = nlp(example_sentence)\n",
        "\n",
        "print(\"\\nExample Sentence:\")\n",
        "print(example_sentence)\n",
        "\n",
        "print(\"\\nDependency Parsing:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:15} -> {token.dep_:10} -> {token.head.text}\")\n",
        "\n",
        "# Constituency Parsing\n",
        "sent = list(doc.sents)[0]\n",
        "tree = sent._.parse_string\n",
        "print(\"\\nConstituency Parse Tree:\")\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI9yqkZ3qvQc",
        "outputId": "95ebac12-4020-40d2-a123-a0522a182a97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Sentence:\n",
            "rent curiousyellow video store controversi surround first releas also heard first seiz u custom ever tri enter countri therefor fan film consid controversi realli see myselfbr br plot center around young swedish drama student name lena want learn everyth life particular want focu attent make sort documentari averag swede thought certain polit issu vietnam war race issu unit state ask politician ordinari denizen stockholm opinion polit sex drama teacher classmat marri menbr br kill curiousyellow year ago consid pornograph realli sex nuditi scene far even shot like cheapli made porno countryman mind find shock realiti sex nuditi major stapl swedish cinema even ingmar bergman arguabl answer good old boy john ford sex scene filmsbr br commend filmmak fact sex shown film shown artist purpos rather shock peopl make money shown pornograph theater america curiousyellow good film anyon want studi meat potato pun intend swedish cinema realli film doesnt much plot\n",
            "\n",
            "Dependency Parsing:\n",
            "rent            -> compound   -> store\n",
            "curiousyellow   -> compound   -> store\n",
            "video           -> compound   -> store\n",
            "store           -> compound   -> controversi\n",
            "controversi     -> nsubj      -> surround\n",
            "surround        -> nsubj      -> heard\n",
            "first           -> amod       -> releas\n",
            "releas          -> dobj       -> surround\n",
            "also            -> advmod     -> heard\n",
            "heard           -> ROOT       -> heard\n",
            "first           -> amod       -> seiz\n",
            "seiz            -> compound   -> custom\n",
            "u               -> compound   -> custom\n",
            "custom          -> nsubj      -> enter\n",
            "ever            -> advmod     -> tri\n",
            "tri             -> appos      -> custom\n",
            "enter           -> ccomp      -> heard\n",
            "countri         -> amod       -> realli\n",
            "therefor        -> prep       -> countri\n",
            "fan             -> compound   -> film\n",
            "film            -> pobj       -> therefor\n",
            "consid          -> compound   -> realli\n",
            "controversi     -> compound   -> realli\n",
            "realli          -> dobj       -> enter\n",
            "see             -> conj       -> heard\n",
            "myselfbr        -> compound   -> center\n",
            "br              -> compound   -> center\n",
            "plot            -> compound   -> center\n",
            "center          -> nsubj      -> want\n",
            "around          -> prep       -> center\n",
            "young           -> amod       -> lena\n",
            "swedish         -> amod       -> lena\n",
            "drama           -> compound   -> student\n",
            "student         -> compound   -> name\n",
            "name            -> compound   -> lena\n",
            "lena            -> pobj       -> around\n",
            "want            -> ccomp      -> see\n",
            "learn           -> xcomp      -> want\n",
            "everyth         -> compound   -> life\n",
            "life            -> nsubj      -> want\n",
            "particular      -> amod       -> life\n",
            "want            -> ccomp      -> learn\n",
            "focu            -> compound   -> attent\n",
            "attent          -> nsubj      -> make\n",
            "make            -> ccomp      -> want\n",
            "sort            -> dobj       -> make\n",
            "documentari     -> compound   -> swede\n",
            "averag          -> compound   -> swede\n",
            "swede           -> nsubj      -> thought\n",
            "thought         -> ccomp      -> want\n",
            "certain         -> amod       -> polit\n",
            "polit           -> nmod       -> ordinari\n",
            "issu            -> nmod       -> race\n",
            "vietnam         -> compound   -> war\n",
            "war             -> compound   -> race\n",
            "race            -> nmod       -> unit\n",
            "issu            -> compound   -> unit\n",
            "unit            -> compound   -> ordinari\n",
            "state           -> compound   -> ask\n",
            "ask             -> nmod       -> ordinari\n",
            "politician      -> compound   -> ordinari\n",
            "ordinari        -> compound   -> denizen\n",
            "denizen         -> compound   -> stockholm\n",
            "stockholm       -> compound   -> polit\n",
            "opinion         -> compound   -> polit\n",
            "polit           -> amod       -> br\n",
            "sex             -> compound   -> drama\n",
            "drama           -> compound   -> teacher\n",
            "teacher         -> nmod       -> marri\n",
            "classmat        -> compound   -> marri\n",
            "marri           -> compound   -> br\n",
            "menbr           -> compound   -> br\n",
            "br              -> nsubj      -> kill\n",
            "kill            -> ccomp      -> thought\n",
            "curiousyellow   -> compound   -> year\n",
            "year            -> npadvmod   -> ago\n",
            "ago             -> advmod     -> made\n",
            "consid          -> amod       -> scene\n",
            "pornograph      -> compound   -> realli\n",
            "realli          -> compound   -> nuditi\n",
            "sex             -> compound   -> nuditi\n",
            "nuditi          -> compound   -> scene\n",
            "scene           -> nsubj      -> made\n",
            "far             -> advmod     -> even\n",
            "even            -> advmod     -> shot\n",
            "shot            -> acl        -> scene\n",
            "like            -> prep       -> shot\n",
            "cheapli         -> pobj       -> like\n",
            "made            -> advcl      -> heard\n",
            "porno           -> compound   -> countryman\n",
            "countryman      -> compound   -> mind\n",
            "mind            -> nsubj      -> find\n",
            "find            -> ccomp      -> made\n",
            "shock           -> compound   -> realiti\n",
            "realiti         -> poss       -> stapl\n",
            "sex             -> compound   -> nuditi\n",
            "nuditi          -> appos      -> realiti\n",
            "major           -> amod       -> stapl\n",
            "stapl           -> nsubj      -> cinema\n",
            "swedish         -> compound   -> cinema\n",
            "cinema          -> ccomp      -> find\n",
            "even            -> advmod     -> ingmar\n",
            "ingmar          -> nmod       -> answer\n",
            "bergman         -> nmod       -> answer\n",
            "arguabl         -> amod       -> answer\n",
            "answer          -> appos      -> cinema\n",
            "good            -> amod       -> boy\n",
            "old             -> amod       -> boy\n",
            "boy             -> appos      -> answer\n",
            "john            -> compound   -> ford\n",
            "ford            -> compound   -> scene\n",
            "sex             -> compound   -> scene\n",
            "scene           -> appos      -> boy\n",
            "filmsbr         -> compound   -> br\n",
            "br              -> compound   -> commend\n",
            "commend         -> compound   -> film\n",
            "filmmak         -> compound   -> fact\n",
            "fact            -> nmod       -> film\n",
            "sex             -> npadvmod   -> shown\n",
            "shown           -> amod       -> film\n",
            "film            -> nsubj      -> shown\n",
            "shown           -> conj       -> heard\n",
            "artist          -> compound   -> purpos\n",
            "purpos          -> nsubj      -> make\n",
            "rather          -> advmod     -> peopl\n",
            "shock           -> compound   -> peopl\n",
            "peopl           -> appos      -> purpos\n",
            "make            -> ccomp      -> shown\n",
            "money           -> dobj       -> make\n",
            "shown           -> advcl      -> heard\n",
            "pornograph      -> compound   -> theater\n",
            "theater         -> compound   -> curiousyellow\n",
            "america         -> compound   -> curiousyellow\n",
            "curiousyellow   -> nmod       -> anyon\n",
            "good            -> amod       -> anyon\n",
            "film            -> compound   -> anyon\n",
            "anyon           -> nsubj      -> want\n",
            "want            -> conj       -> heard\n",
            "studi           -> compound   -> potato\n",
            "meat            -> compound   -> potato\n",
            "potato          -> compound   -> pun\n",
            "pun             -> nsubj      -> intend\n",
            "intend          -> ccomp      -> want\n",
            "swedish         -> amod       -> film\n",
            "cinema          -> compound   -> realli\n",
            "realli          -> compound   -> film\n",
            "film            -> dobj       -> intend\n",
            "does            -> aux        -> plot\n",
            "nt              -> neg        -> plot\n",
            "much            -> amod       -> plot\n",
            "plot            -> punct      -> heard\n",
            "\n",
            "Constituency Parse Tree:\n",
            "(SINV (VB rent) (NP (NP (JJ curiousyellow) (NN video) (NN store) (NNS controversi)) (VBP surround) (NP (JJ first) (FW releas))) (ADVP (ADVP (RB also)) (VBN heard) (JJ first) (FW seiz) (NP (RB u)) (RB custom) (ADVP (RB ever))) (VB tri) (VB enter) (ADVP (NP (NN countri)) (ADVP (RB therefor))) (NP (NP (NP (NN fan) (NN film)) (FW consid) (NNS controversi)) (FW realli)) (VP (VB see) (NP (FW myselfbr) (RB br) (NN plot)) (NN center) (IN around) (NP (NML (JJ young) (JJ swedish) (NN drama) (NN student)) (NN name) (FW lena)) (VBP want) (S (VP (VB learn) (NP (JJ everyth) (NN life)))) (JJ particular) (VBP want) (NP (NP (FW focu)) (RB attent)) (VP (VP (VB make) (NP (NN sort)) (RB documentari)) (NP (NN averag) (JJ swede)) (VP (VBD thought) (SBAR (S (JJ certain) (NN polit) (FW issu) (NNP vietnam) (NN war) (NN race) (IN issu) (NN unit) (NN state) (VB ask) (NP (NP (NN politician) (JJ ordinari) (. denizen) (NN stockholm) (NN opinion) (NN polit) (NML (NML (NN sex) (NN drama)) (NN teacher)) (ADVP (NN classmat) (ADVP (RB marri) (FW menbr) (ADVP (UH br) (PP (VB kill) (ADVP (NN curiousyellow) (ADVP (NP (NN year)) (RB ago))))))) (NN consid) (NN pornograph) (NN realli) (NN sex) (. nuditi) (NN scene)) (ADVP (RB far) (RB even)) (VB shot) (PP (PP (IN like) (NP (NP (NN cheapli)) (VBN made) (ADVP (FW porno)) (NN countryman) (NN mind))) (VP (VB find) (NP (NP (NP (NP (NP (NP (NP (NN shock)) (VB realiti) (NP (NN sex) (NN nuditi) (JJ major) (NN stapl) (JJ swedish) (NN cinema))) (ADVP (RB even)) (JJ ingmar) (NN bergman)) (VB arguabl) (NN answer) (NML (JJ good) (JJ old) (NN boy))) (NN john) (VBN ford) (NN sex) (NN scene)) (VB filmsbr) (RB br)) (VP (VB commend) (NP (JJ filmmak) (NP (NN fact) (SBAR (S (NP (NP (NP (NP (ADJP (NP (NN sex)) (VBN shown)) (NN film)) (VBN shown) (NN artist)) (RB purpos)) (ADVP (RB rather)) (NP (NN shock) (NP (JJ peopl)))) (VP (VB make) (NP (NN money)) (VP (VBN shown) (NP (NP (NN pornograph) (NN theater) (NNP america)) (JJ curiousyellow) (JJ good) (. film) (RB anyon) (VP (VB want) (NP (FW studi) (NN meat) (NN potato) (. pun) (RB intend) (NP (NP (NP (JJ swedish) (NN cinema)) (. realli) (NN film)) (VP (VBZ does) (RB nt) (NP (JJ much) (NN plot))))))))))))))))))))))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/distributions/distribution.py:62: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition"
      ],
      "metadata": {
        "id": "mnvZG4jTqvfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity_counts = Counter()\n",
        "\n",
        "for review in sample_reviews:\n",
        "    doc = nlp(review)\n",
        "    for ent in doc.ents:\n",
        "        entity_counts[ent.label_] += 1\n",
        "\n",
        "print(\"Named Entity Counts:\")\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc23gAx0umW1",
        "outputId": "f443110d-96c7-4eae-bc2f-3580c9869c96"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Counts:\n",
            "Counter({'PERSON': 683, 'CARDINAL': 256, 'ORG': 200, 'NORP': 152, 'GPE': 73, 'DATE': 61, 'ORDINAL': 54, 'TIME': 30, 'LOC': 15, 'FAC': 8, 'PRODUCT': 2, 'QUANTITY': 2, 'EVENT': 1, 'MONEY': 1, 'LAW': 1, 'LANGUAGE': 1, 'WORK_OF_ART': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "iECkfGgRu0l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)\n",
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "Og47oLI6u11D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: Collect ~1000 GitHub Marketplace Actions-like products (via GitHub Search API)\n",
        "# Paste and run this cell in Google Colab.\n",
        "\n",
        "!pip install pandas requests tqdm\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# ---------- USER CONFIG ----------\n",
        "# Maximum items to collect (1000 is typical API search cap per query)\n",
        "MAX_ITEMS = 1000\n",
        "\n",
        "# Results per page (GitHub allows up to 100)\n",
        "PER_PAGE = 100\n",
        "\n",
        "# Delay between requests (seconds)\n",
        "DELAY = 1.0\n",
        "\n",
        "# Search query - we search for repos tagged topic:github-action\n",
        "# You can modify to narrow searches (e.g., add \"topic:action topic:github-marketplace\" or keywords)\n",
        "SEARCH_QUERY = \"topic:github-action\"\n",
        "\n",
        "# ---------------------------\n",
        "# Get GitHub token (recommended)\n",
        "print(\"If you have a GitHub Personal Access Token (recommended for higher rate limits),\")\n",
        "print(\"upload your 'token.txt' file containing the token, or cancel upload to proceed unauthenticated.\")\n",
        "print(\"To create a token: github.com -> Settings -> Developer settings -> Personal access tokens (no scopes needed for public data).\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    # Expect the user to upload a small text file token.txt with the token inside.\n",
        "    token_filename = list(uploaded.keys())[0]\n",
        "    token = uploaded[token_filename].decode('utf-8').strip()\n",
        "    print(\"Token loaded from\", token_filename)\n",
        "    HEADERS = {\"Authorization\": f\"token {token}\", \"Accept\": \"application/vnd.github.v3+json\", \"User-Agent\":\"colab-script\"}\n",
        "except Exception as e:\n",
        "    print(\"No token uploaded. Proceeding unauthenticated (much lower rate limits).\")\n",
        "    HEADERS = {\"Accept\": \"application/vnd.github.v3+json\", \"User-Agent\":\"colab-script\"}\n",
        "\n",
        "# ---------------------------\n",
        "# Helper: fetch page\n",
        "def fetch_search_page(query, page, per_page=100):\n",
        "    url = \"https://api.github.com/search/repositories\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"sort\": \"stars\",\n",
        "        \"order\": \"desc\",\n",
        "        \"per_page\": per_page,\n",
        "        \"page\": page\n",
        "    }\n",
        "    resp = requests.get(url, headers=HEADERS, params=params)\n",
        "    if resp.status_code == 200:\n",
        "        return resp.json()\n",
        "    else:\n",
        "        raise RuntimeError(f\"GitHub API error {resp.status_code}: {resp.text}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Main loop: paginate and collect\n",
        "items = []\n",
        "page = 1\n",
        "collected = 0\n",
        "max_pages = (MAX_ITEMS + PER_PAGE - 1) // PER_PAGE\n",
        "\n",
        "# First request to learn total_count (and to check for caps)\n",
        "print(\"Querying GitHub Search API with:\", SEARCH_QUERY)\n",
        "first = fetch_search_page(SEARCH_QUERY, page, per_page=PER_PAGE)\n",
        "total_count = first.get(\"total_count\", 0)\n",
        "print(\"Search returned total_count =\", total_count, \"(note: GitHub Search API may cap accessible results at 1000).\")\n",
        "\n",
        "# Adjust effective max items based on API cap\n",
        "effective_total = min(total_count, MAX_ITEMS)\n",
        "if total_count > 1000:\n",
        "    print(\"⚠️ GitHub Search API caps accessible results to ~1000 per query. We'll attempt to collect up to\", effective_total)\n",
        "\n",
        "# Process first page results\n",
        "for repo in first.get(\"items\", []):\n",
        "    items.append({\n",
        "        \"name\": repo.get(\"name\"),\n",
        "        \"full_name\": repo.get(\"full_name\"),\n",
        "        \"html_url\": repo.get(\"html_url\"),\n",
        "        \"description\": repo.get(\"description\"),\n",
        "        \"stars\": repo.get(\"stargazers_count\"),\n",
        "        \"language\": repo.get(\"language\"),\n",
        "        \"score\": repo.get(\"score\"),\n",
        "        \"page\": 1\n",
        "    })\n",
        "collected = len(items)\n",
        "print(f\"Collected {collected} so far (page 1).\")\n",
        "\n",
        "# Continue with subsequent pages\n",
        "for page in range(2, max_pages + 1):\n",
        "    if collected >= effective_total:\n",
        "        break\n",
        "    time.sleep(DELAY)  # polite delay\n",
        "    try:\n",
        "        result = fetch_search_page(SEARCH_QUERY, page, per_page=PER_PAGE)\n",
        "    except Exception as e:\n",
        "        print(\"Request failed on page\", page, \":\", e)\n",
        "        break\n",
        "    page_items = result.get(\"items\", [])\n",
        "    if not page_items:\n",
        "        print(\"No more items on page\", page)\n",
        "        break\n",
        "    for repo in page_items:\n",
        "        if collected >= effective_total:\n",
        "            break\n",
        "        items.append({\n",
        "            \"name\": repo.get(\"name\"),\n",
        "            \"full_name\": repo.get(\"full_name\"),\n",
        "            \"html_url\": repo.get(\"html_url\"),\n",
        "            \"description\": repo.get(\"description\"),\n",
        "            \"stars\": repo.get(\"stargazers_count\"),\n",
        "            \"language\": repo.get(\"language\"),\n",
        "            \"score\": repo.get(\"score\"),\n",
        "            \"page\": page\n",
        "        })\n",
        "        collected += 1\n",
        "    print(f\"Collected {collected} so far (up to page {page}).\")\n",
        "\n",
        "print(\"Done collecting. Total items collected:\", len(items))\n",
        "\n",
        "# ---------------------------\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(items)\n",
        "out_filename = \"github_marketplace_actions.csv\"\n",
        "df.to_csv(out_filename, index=False)\n",
        "print(\"Saved to\", out_filename)\n",
        "\n",
        "# Provide a download link (Colab)\n",
        "files.download(out_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "9ux3xhorvCt4",
        "outputId": "ea5df62f-abbb-428c-f11f-94d4906e2939"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "If you have a GitHub Personal Access Token (recommended for higher rate limits),\n",
            "upload your 'token.txt' file containing the token, or cancel upload to proceed unauthenticated.\n",
            "To create a token: github.com -> Settings -> Developer settings -> Personal access tokens (no scopes needed for public data).\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2deb4cfb-8d4e-4f8d-ba8a-a747aff3de29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2deb4cfb-8d4e-4f8d-ba8a-a747aff3de29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No token uploaded. Proceeding unauthenticated (much lower rate limits).\n",
            "Querying GitHub Search API with: topic:github-action\n",
            "Search returned total_count = 2291 (note: GitHub Search API may cap accessible results at 1000).\n",
            "⚠️ GitHub Search API caps accessible results to ~1000 per query. We'll attempt to collect up to 1000\n",
            "Collected 100 so far (page 1).\n",
            "Collected 200 so far (up to page 2).\n",
            "Collected 300 so far (up to page 3).\n",
            "Collected 400 so far (up to page 4).\n",
            "Collected 500 so far (up to page 5).\n",
            "Collected 600 so far (up to page 6).\n",
            "Collected 700 so far (up to page 7).\n",
            "Collected 800 so far (up to page 8).\n",
            "Collected 900 so far (up to page 9).\n",
            "Collected 1000 so far (up to page 10).\n",
            "Done collecting. Total items collected: 1000\n",
            "Saved to github_marketplace_actions.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_14c2a6a5-113c-4252-be30-d647b61641e6\", \"github_marketplace_actions.csv\", 186223)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "TT28nULivhlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "2qfi1KQuwYSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAFf64QEAAAAA9MxJSEFGNBN1DNreu01isKop1dA%3DS5xQKanojRzUtv0vQ9feZln3IO4N2vYTf2jZo9KY4qvXDw1hxr\"  # v2 token\n",
        "\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "hashtags = [\"#MachineLearning\", \"#ArtificialIntelligence\"]\n",
        "tweets_data = []\n",
        "\n",
        "for tag in hashtags:\n",
        "    query = f\"{tag} -is:retweet lang:en\"  # exclude retweets, only English\n",
        "    try:\n",
        "        response = client.search_recent_tweets(\n",
        "            query=query,\n",
        "            tweet_fields=[\"id\", \"text\", \"author_id\", \"created_at\"],\n",
        "            max_results=10  # safe number to avoid 429\n",
        "        )\n",
        "\n",
        "        if response.data:\n",
        "            for tweet in response.data:\n",
        "                tweets_data.append({\n",
        "                    \"tweet_id\": tweet.id,\n",
        "                    \"user_id\": tweet.author_id,\n",
        "                    \"text\": tweet.text,\n",
        "                    \"created_at\": tweet.created_at\n",
        "                })\n",
        "        # pause 30 seconds to respect rate limits\n",
        "        time.sleep(30)\n",
        "    except tweepy.TooManyRequests:\n",
        "        print(\"Rate limit hit. Waiting 60 seconds before continuing...\")\n",
        "        time.sleep(60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcFwIqO3wYCS",
        "outputId": "0aa0a231-f6cf-4a52-8bf0-51cef4463f65"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit. Waiting 60 seconds before continuing...\n",
            "Rate limit hit. Waiting 60 seconds before continuing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(tweets_data)\n",
        "print(f\"Total tweets collected: {len(df)}\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9UXAjf_5cvS",
        "outputId": "01798217-50e3-4bda-d527-dc51739f4eea"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tweets collected: 0\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51e4244a",
        "outputId": "a50b4c71-f21b-4898-fbbe-75278abc89bf"
      },
      "source": [
        "# ================================\n",
        "# Q5 - PART 1: Collect Tweets (using Tweepy v2)\n",
        "# ================================\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import os\n",
        "import time # Import time for delays\n",
        "\n",
        "# --- Step 1: Set up Twitter API v2 authentication ---\n",
        "# Using the bearer token provided by the user\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAFf64QEAAAAA9MxJSEFGNBN1DNreu01isKop1dA%3DS5xQKanojRzUtv0vQ9feZln3IO4N2vYTf2jZo9KY4qvXDw1hxr\"\n",
        "\n",
        "client = tweepy.Client(bearer_token)\n",
        "\n",
        "hashtags = [\"#MachineLearning\", \"#ArtificialIntelligence\"]\n",
        "\n",
        "# Create empty list for storing tweets\n",
        "tweets_data = []\n",
        "\n",
        "# Scrape tweets using v2 API's search_recent_tweets\n",
        "# We can fetch up to 100 tweets per request using max_results\n",
        "# To get more, we would need to paginate using next_token,\n",
        "# but for a simple example, we'll limit to 100 per hashtag for now.\n",
        "total_tweets_to_fetch_per_hashtag = 100 # You can increase this, but consider API rate limits and pagination\n",
        "\n",
        "print(f\"Fetching up to {total_tweets_to_fetch_per_hashtag} recent tweets per hashtag...\")\n",
        "\n",
        "for tag in hashtags:\n",
        "    print(f\"\\nFetching tweets for hashtag: {tag}\")\n",
        "    try:\n",
        "        # Use search_recent_tweets from the v2 API\n",
        "        # max_results can be between 10 and 100\n",
        "        response = client.search_recent_tweets(tag, max_results=100)\n",
        "        if response.data:\n",
        "            for tweet in response.data:\n",
        "                tweets_data.append({\n",
        "                    \"tweet_id\": tweet.id,\n",
        "                    \"text\": tweet.text\n",
        "                    # Note: v2 API gives tweet.id and tweet.text directly.\n",
        "                    # To get username, you'd need to use expansions='author_id'\n",
        "                    # and process includes['users']. Keeping it simple for now.\n",
        "                })\n",
        "            print(f\"Fetched {len(response.data)} tweets for {tag}.\")\n",
        "        else:\n",
        "            print(f\"No recent tweets found for {tag}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching tweets for {tag}: {e}\")\n",
        "\n",
        "    # Add a small delay between requests to avoid hitting rate limits quickly\n",
        "    time.sleep(5)\n",
        "\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_tweets = pd.DataFrame(tweets_data)\n",
        "\n",
        "if not df_tweets.empty:\n",
        "    # Add a dummy username column for consistency with the original code structure\n",
        "    # If you need actual usernames, you'd need to use expansions='author_id' in client.search_recent_tweets\n",
        "    df_tweets['username'] = 'N/A' # Or fetch usernames using expansions\n",
        "\n",
        "    print(\"\\nCollected Tweets:\")\n",
        "    print(df_tweets.head())\n",
        "\n",
        "    # Save to CSV\n",
        "    out_filename = \"tweets_raw.csv\"\n",
        "    df_tweets.to_csv(out_filename, index=False)\n",
        "    print(f\"\\nSaved collected tweets to {out_filename}\")\n",
        "\n",
        "    # Provide a download link (Colab)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(out_filename)\n",
        "    except ImportError:\n",
        "        print(f\"Please download the file '{out_filename}' manually.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo tweets were collected. Data cleaning will not proceed.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching up to 100 recent tweets per hashtag...\n",
            "\n",
            "Fetching tweets for hashtag: #MachineLearning\n",
            "Error fetching tweets for #MachineLearning: 429 Too Many Requests\n",
            "Too Many Requests\n",
            "\n",
            "Fetching tweets for hashtag: #ArtificialIntelligence\n",
            "Error fetching tweets for #ArtificialIntelligence: 429 Too Many Requests\n",
            "Too Many Requests\n",
            "\n",
            "No tweets were collected. Data cleaning will not proceed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "ypuz5j8p6wQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "dgA9HF657bE5"
      }
    }
  ]
}